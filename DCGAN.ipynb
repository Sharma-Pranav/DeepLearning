{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sharma-Pranav/DeepLearning/blob/master/DCGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "t9BDJ3ryunBI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "c1c21861-c7f5-4f1d-ed78-fd34132d3083"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "\n",
        "latent_dim = 32\n",
        "height = 32\n",
        "width = 32\n",
        "channels = 3\n",
        "\n",
        "generator_input = keras.Input(shape=(latent_dim,))\n",
        "\n",
        "# First, transform the input into a 16x16 128-channels feature map\n",
        "x = layers.Dense(128 * 16 * 16)(generator_input)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Reshape((16, 16, 128))(x)\n",
        "\n",
        "# Then, add a convolution layer\n",
        "x = layers.Conv2D(256, 5, padding='same')(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "\n",
        "# Upsample to 32x32\n",
        "x = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "\n",
        "# Few more conv layers\n",
        "x = layers.Conv2D(256, 5, padding='same')(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Conv2D(256, 5, padding='same')(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "\n",
        "# Produce a 32x32 1-channel feature map\n",
        "x = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)\n",
        "generator = keras.models.Model(generator_input, x)\n",
        "generator.summary()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32768)             1081344   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 32768)             0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 16, 16, 256)       819456    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 256)       1048832   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 32, 32, 3)         37635     \n",
            "=================================================================\n",
            "Total params: 6,264,579\n",
            "Trainable params: 6,264,579\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FQB8zraF8Po9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "6637e761-c9b7-4d1b-e02f-3a1da30d8c89"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "discriminator_input = layers.Input(shape=(height, width, channels))\n",
        "x = layers.Conv2D(128, 3)(discriminator_input)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Conv2D(128, 4, strides=2)(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Conv2D(128, 4, strides=2)(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Conv2D(128, 4, strides=2)(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Flatten()(x)\n",
        "\n",
        "# One dropout layer - important trick!\n",
        "x = layers.Dropout(0.4)(x)\n",
        "\n",
        "# Classification layer\n",
        "x = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "discriminator = keras.models.Model(discriminator_input, x)\n",
        "discriminator.summary()\n",
        "\n",
        "# To stabilize training, we use learning rate decay\n",
        "# and gradient clipping (by value) in the optimizer.\n",
        "discriminator_optimizer = keras.optimizers.RMSprop(lr=0.0008, clipvalue=1.0, decay=1e-8)\n",
        "discriminator.compile(optimizer=discriminator_optimizer, loss='binary_crossentropy')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 30, 30, 128)       3584      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 30, 30, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 14, 14, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 6, 6, 128)         262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 2, 2, 128)         262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 790,913\n",
            "Trainable params: 790,913\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jdxAb84K8gcc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Set discriminator weights to non-trainable\n",
        "# (will only apply to the `gan` model)\n",
        "discriminator.trainable = False\n",
        "\n",
        "gan_input = keras.Input(shape=(latent_dim,))\n",
        "gan_output = discriminator(generator(gan_input))\n",
        "gan = keras.models.Model(gan_input, gan_output)\n",
        "\n",
        "gan_optimizer = keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8)\n",
        "gan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FABTj5QjB6IU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b6dc6b18-a2b6-404f-9790-f86e41449773"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from keras.preprocessing import image\n",
        "\n",
        "# Load Data\n",
        "(x_train, y_train), (_, _) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Select frog images (class 6)\n",
        "x_train = x_train[y_train.flatten() == 6]\n",
        "\n",
        "# Normalize data\n",
        "x_train = x_train.reshape((x_train.shape[0],) + (height, width, channels)).astype('float32') / 255.\n",
        "\n",
        "iterations = 10000\n",
        "batch_size = 20\n",
        "path = os.getcwd()\n",
        "save_dir = os.path.join(path, 'gan_images')\n",
        "\n",
        "# Create target directory & all intermediate directories if don't exists\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "    print(\"Directory \" , save_dir ,  \" Created \")\n",
        "else:    \n",
        "    print(\"Directory \" , save_dir ,  \" already exists\")   "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 59s 0us/step\n",
            "Directory  /content/gan_images  Created \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0geTffDDOSEx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3522
        },
        "outputId": "c0be8680-7e90-45db-f58e-fa3d5c3af550"
      },
      "cell_type": "code",
      "source": [
        "# Start training loop\n",
        "start = 0\n",
        "for step in range(iterations):\n",
        "    # Sample random points in the latent space\n",
        "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
        "\n",
        "    # Decode them to fake images\n",
        "    generated_images = generator.predict(random_latent_vectors)\n",
        "\n",
        "    # Combine them with real images\n",
        "    stop = start + batch_size\n",
        "    real_images = x_train[start: stop]\n",
        "    combined_images = np.concatenate([generated_images, real_images])\n",
        "\n",
        "    # Assemble labels discriminating real from fake images\n",
        "    labels = np.concatenate([np.ones((batch_size, 1)),\n",
        "                             np.zeros((batch_size, 1))])\n",
        "    # Add random noise to the labels - important trick!\n",
        "    labels += 0.05 * np.random.random(labels.shape)\n",
        "\n",
        "    # Train the discriminator\n",
        "    d_loss = discriminator.train_on_batch(combined_images, labels)\n",
        "\n",
        "    # sample random points in the latent space\n",
        "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
        "\n",
        "    # Assemble labels that say \"all real images\"\n",
        "    misleading_targets = np.zeros((batch_size, 1))\n",
        "\n",
        "    # Train the generator (via the gan model,\n",
        "    # where the discriminator weights are frozen)\n",
        "    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
        "    \n",
        "    start += batch_size\n",
        "    if start > len(x_train) - batch_size:\n",
        "      start = 0\n",
        "\n",
        "    # Occasionally save / plot\n",
        "    if step % 100 == 0:\n",
        "        # Save model weights\n",
        "        gan.save_weights('gan.h5')\n",
        "\n",
        "        # Print metrics\n",
        "        print('discriminator loss at step %s: %s' % (step, d_loss))\n",
        "        print('adversarial loss at step %s: %s' % (step, a_loss))\n",
        "\n",
        "        # Save one generated image\n",
        "        img = image.array_to_img(generated_images[0] * 255., scale=False)\n",
        "        img.save(os.path.join(save_dir, 'generated_frog' + str(step) + '.png'))\n",
        "\n",
        "        # Save one real image, for comparison\n",
        "        img = image.array_to_img(real_images[0] * 255., scale=False)\n",
        "        img.save(os.path.join(save_dir, 'real_frog' + str(step) + '.png'))\n",
        "  "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "discriminator loss at step 0: 0.6873592\n",
            "adversarial loss at step 0: 0.6609336\n",
            "discriminator loss at step 100: 0.7045557\n",
            "adversarial loss at step 100: 0.82043725\n",
            "discriminator loss at step 200: 0.6713661\n",
            "adversarial loss at step 200: 0.83787143\n",
            "discriminator loss at step 300: 0.6951877\n",
            "adversarial loss at step 300: 0.76632106\n",
            "discriminator loss at step 400: 0.69183\n",
            "adversarial loss at step 400: 0.81275827\n",
            "discriminator loss at step 500: 0.68070364\n",
            "adversarial loss at step 500: 0.6948966\n",
            "discriminator loss at step 600: 0.68975824\n",
            "adversarial loss at step 600: 0.7341875\n",
            "discriminator loss at step 700: 0.69879925\n",
            "adversarial loss at step 700: 0.7574623\n",
            "discriminator loss at step 800: 0.7100836\n",
            "adversarial loss at step 800: 0.74758023\n",
            "discriminator loss at step 900: 0.69781935\n",
            "adversarial loss at step 900: 0.7636054\n",
            "discriminator loss at step 1000: 0.69090706\n",
            "adversarial loss at step 1000: 0.7420332\n",
            "discriminator loss at step 1100: 0.6840423\n",
            "adversarial loss at step 1100: 0.73053783\n",
            "discriminator loss at step 1200: 0.7177084\n",
            "adversarial loss at step 1200: 0.757638\n",
            "discriminator loss at step 1300: 0.9253613\n",
            "adversarial loss at step 1300: 0.9452671\n",
            "discriminator loss at step 1400: 0.7046472\n",
            "adversarial loss at step 1400: 0.7336547\n",
            "discriminator loss at step 1500: 0.7065569\n",
            "adversarial loss at step 1500: 0.78043807\n",
            "discriminator loss at step 1600: 0.6992111\n",
            "adversarial loss at step 1600: 0.76564646\n",
            "discriminator loss at step 1700: 0.69897354\n",
            "adversarial loss at step 1700: 0.71878207\n",
            "discriminator loss at step 1800: 0.6908154\n",
            "adversarial loss at step 1800: 0.7111651\n",
            "discriminator loss at step 1900: 0.7030299\n",
            "adversarial loss at step 1900: 0.734547\n",
            "discriminator loss at step 2000: 0.6847644\n",
            "adversarial loss at step 2000: 0.725304\n",
            "discriminator loss at step 2100: 0.68877953\n",
            "adversarial loss at step 2100: 0.75727826\n",
            "discriminator loss at step 2200: 0.6856912\n",
            "adversarial loss at step 2200: 1.1161845\n",
            "discriminator loss at step 2300: 0.69984454\n",
            "adversarial loss at step 2300: 0.72184753\n",
            "discriminator loss at step 2400: 0.6971818\n",
            "adversarial loss at step 2400: 0.723776\n",
            "discriminator loss at step 2500: 0.69744253\n",
            "adversarial loss at step 2500: 0.73160756\n",
            "discriminator loss at step 2600: 0.7080587\n",
            "adversarial loss at step 2600: 0.75192755\n",
            "discriminator loss at step 2700: 0.69138443\n",
            "adversarial loss at step 2700: 0.8767419\n",
            "discriminator loss at step 2800: 0.7045766\n",
            "adversarial loss at step 2800: 0.86851805\n",
            "discriminator loss at step 2900: 0.6928891\n",
            "adversarial loss at step 2900: 0.7125598\n",
            "discriminator loss at step 3000: 0.7035066\n",
            "adversarial loss at step 3000: 0.740737\n",
            "discriminator loss at step 3100: 0.6846718\n",
            "adversarial loss at step 3100: 0.73592424\n",
            "discriminator loss at step 3200: 0.7325315\n",
            "adversarial loss at step 3200: 0.7679028\n",
            "discriminator loss at step 3300: 0.66832113\n",
            "adversarial loss at step 3300: 0.4753043\n",
            "discriminator loss at step 3400: 0.69268453\n",
            "adversarial loss at step 3400: 0.7791976\n",
            "discriminator loss at step 3500: 0.69168377\n",
            "adversarial loss at step 3500: 0.75851065\n",
            "discriminator loss at step 3600: 0.69112563\n",
            "adversarial loss at step 3600: 0.7369287\n",
            "discriminator loss at step 3700: 0.6867317\n",
            "adversarial loss at step 3700: 0.81248796\n",
            "discriminator loss at step 3800: 0.6901283\n",
            "adversarial loss at step 3800: 0.7600374\n",
            "discriminator loss at step 3900: 0.7120476\n",
            "adversarial loss at step 3900: 0.7124923\n",
            "discriminator loss at step 4000: 0.70479864\n",
            "adversarial loss at step 4000: 0.77207196\n",
            "discriminator loss at step 4100: 0.71376103\n",
            "adversarial loss at step 4100: 0.7227291\n",
            "discriminator loss at step 4200: 0.69124293\n",
            "adversarial loss at step 4200: 0.7386012\n",
            "discriminator loss at step 4300: 0.88299733\n",
            "adversarial loss at step 4300: 0.6756333\n",
            "discriminator loss at step 4400: 0.6975055\n",
            "adversarial loss at step 4400: 0.7539247\n",
            "discriminator loss at step 4500: 1.0898019\n",
            "adversarial loss at step 4500: 0.7932539\n",
            "discriminator loss at step 4600: 0.69154245\n",
            "adversarial loss at step 4600: 0.7118797\n",
            "discriminator loss at step 4700: 0.69129246\n",
            "adversarial loss at step 4700: 0.7601611\n",
            "discriminator loss at step 4800: 0.6948495\n",
            "adversarial loss at step 4800: 0.7580651\n",
            "discriminator loss at step 4900: 0.72099096\n",
            "adversarial loss at step 4900: 0.8064327\n",
            "discriminator loss at step 5000: 0.69584405\n",
            "adversarial loss at step 5000: 0.7438041\n",
            "discriminator loss at step 5100: 0.69445515\n",
            "adversarial loss at step 5100: 0.72286904\n",
            "discriminator loss at step 5200: 0.691846\n",
            "adversarial loss at step 5200: 0.75536984\n",
            "discriminator loss at step 5300: 0.69620746\n",
            "adversarial loss at step 5300: 0.75591755\n",
            "discriminator loss at step 5400: 0.7285164\n",
            "adversarial loss at step 5400: 0.6969885\n",
            "discriminator loss at step 5500: 0.6995261\n",
            "adversarial loss at step 5500: 0.7672446\n",
            "discriminator loss at step 5600: 0.68389785\n",
            "adversarial loss at step 5600: 0.597787\n",
            "discriminator loss at step 5700: 0.69790375\n",
            "adversarial loss at step 5700: 0.7797369\n",
            "discriminator loss at step 5800: 0.69576156\n",
            "adversarial loss at step 5800: 0.76284903\n",
            "discriminator loss at step 5900: 0.69600266\n",
            "adversarial loss at step 5900: 0.7359117\n",
            "discriminator loss at step 6000: 0.69548583\n",
            "adversarial loss at step 6000: 0.7316346\n",
            "discriminator loss at step 6100: 0.68540794\n",
            "adversarial loss at step 6100: 0.75999624\n",
            "discriminator loss at step 6200: 0.70457476\n",
            "adversarial loss at step 6200: 0.7791003\n",
            "discriminator loss at step 6300: 0.7010682\n",
            "adversarial loss at step 6300: 0.7062001\n",
            "discriminator loss at step 6400: 0.7514208\n",
            "adversarial loss at step 6400: 0.74848604\n",
            "discriminator loss at step 6500: 0.68952\n",
            "adversarial loss at step 6500: 0.7713249\n",
            "discriminator loss at step 6600: 0.69543856\n",
            "adversarial loss at step 6600: 0.7429633\n",
            "discriminator loss at step 6700: 0.708181\n",
            "adversarial loss at step 6700: 0.7412189\n",
            "discriminator loss at step 6800: 0.71692836\n",
            "adversarial loss at step 6800: 0.7506454\n",
            "discriminator loss at step 6900: 0.698845\n",
            "adversarial loss at step 6900: 0.7409165\n",
            "discriminator loss at step 7000: 0.6907426\n",
            "adversarial loss at step 7000: 0.72904444\n",
            "discriminator loss at step 7100: 0.69418484\n",
            "adversarial loss at step 7100: 0.72642076\n",
            "discriminator loss at step 7200: 0.7026644\n",
            "adversarial loss at step 7200: 0.80833036\n",
            "discriminator loss at step 7300: 0.70103157\n",
            "adversarial loss at step 7300: 0.6892828\n",
            "discriminator loss at step 7400: 0.6916905\n",
            "adversarial loss at step 7400: 0.7454323\n",
            "discriminator loss at step 7500: 0.69830024\n",
            "adversarial loss at step 7500: 0.8454251\n",
            "discriminator loss at step 7600: 0.69939375\n",
            "adversarial loss at step 7600: 0.7324398\n",
            "discriminator loss at step 7700: 0.6991388\n",
            "adversarial loss at step 7700: 0.8771993\n",
            "discriminator loss at step 7800: 0.6950819\n",
            "adversarial loss at step 7800: 0.7315401\n",
            "discriminator loss at step 7900: 0.69278204\n",
            "adversarial loss at step 7900: 0.75940067\n",
            "discriminator loss at step 8000: 0.69182026\n",
            "adversarial loss at step 8000: 0.74354243\n",
            "discriminator loss at step 8100: 0.68308264\n",
            "adversarial loss at step 8100: 0.80033875\n",
            "discriminator loss at step 8200: 0.69438547\n",
            "adversarial loss at step 8200: 0.773222\n",
            "discriminator loss at step 8300: 0.70410234\n",
            "adversarial loss at step 8300: 0.76001215\n",
            "discriminator loss at step 8400: 0.69455117\n",
            "adversarial loss at step 8400: 0.7574934\n",
            "discriminator loss at step 8500: 0.7048825\n",
            "adversarial loss at step 8500: 0.8124136\n",
            "discriminator loss at step 8600: 0.6957865\n",
            "adversarial loss at step 8600: 0.7543389\n",
            "discriminator loss at step 8700: 0.7320681\n",
            "adversarial loss at step 8700: 0.74695766\n",
            "discriminator loss at step 8800: 0.68162185\n",
            "adversarial loss at step 8800: 0.74404585\n",
            "discriminator loss at step 8900: 0.69671226\n",
            "adversarial loss at step 8900: 0.76154435\n",
            "discriminator loss at step 9000: 0.69661254\n",
            "adversarial loss at step 9000: 0.74601233\n",
            "discriminator loss at step 9100: 0.6922613\n",
            "adversarial loss at step 9100: 0.7802712\n",
            "discriminator loss at step 9200: 0.6852155\n",
            "adversarial loss at step 9200: 0.728933\n",
            "discriminator loss at step 9300: 0.69529927\n",
            "adversarial loss at step 9300: 0.86784935\n",
            "discriminator loss at step 9400: 0.7034044\n",
            "adversarial loss at step 9400: 0.7081213\n",
            "discriminator loss at step 9500: 0.687472\n",
            "adversarial loss at step 9500: 0.75129116\n",
            "discriminator loss at step 9600: 0.67949545\n",
            "adversarial loss at step 9600: 0.7472292\n",
            "discriminator loss at step 9700: 0.70488083\n",
            "adversarial loss at step 9700: 0.8415656\n",
            "discriminator loss at step 9800: 0.73704076\n",
            "adversarial loss at step 9800: 0.7854607\n",
            "discriminator loss at step 9900: 0.69591635\n",
            "adversarial loss at step 9900: 0.7978816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PuGQ_4VCO8jf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample random points in the latent space\n",
        "random_latent_vectors = np.random.normal(size=(10, latent_dim))\n",
        "\n",
        "# Decode them to fake images\n",
        "generated_images = generator.predict(random_latent_vectors)\n",
        "\n",
        "for i in range(generated_images.shape[0]):\n",
        "    img = image.array_to_img(generated_images[i] * 255., scale=False)\n",
        "    plt.figure()\n",
        "    plt.imshow(img)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yf1164F_agEe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}